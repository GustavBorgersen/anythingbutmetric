name: Daily scraper

on:
  schedule:
    - cron: '0 6 * * *'   # 06:00 UTC daily
  workflow_dispatch:        # manual trigger for testing
    inputs:
      clear_scraped:
        description: 'Wipe live data before running (fresh test — resets edges.json to [])'
        type: boolean
        default: false
      filter_both_new:
        description: 'Reject edges where both sides are new units (enable once catalogue is large)'
        type: boolean
        default: false
      max_age_hours:
        description: 'Max article age in hours (0 = no filter; use 0 when adding a new feed to backfill)'
        type: number
        default: 26

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      - name: Install dependencies
        run: pip install -r scraper/requirements.txt

      - name: Clear scraped data (test reset)
        if: ${{ github.event.inputs.clear_scraped == 'true' }}
        run: |
          echo "[]" > data/edges.json
          cp data/seed-units.json data/units.json

      - name: Run scraper
        id: run_scraper
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          GOOGLE_AI_API_KEY: ${{ secrets.GOOGLE_AI_API_KEY }}
        run: |
          FLAGS=""
          if [ "${{ github.event.inputs.filter_both_new }}" = "true" ]; then
            FLAGS="$FLAGS --filter-both-new"
          fi
          FLAGS="$FLAGS --max-age-hours ${{ github.event.inputs.max_age_hours || '26' }}"
          output=$(python scraper/scraper.py $FLAGS)
          echo "$output"
          echo "$output" >> $GITHUB_OUTPUT

      - name: Open PR if new edges found
        if: ${{ steps.run_scraper.outputs.NEW_EDGES != '0' }}
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          BRANCH="scraper/$(date +%Y-%m-%d)"
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git checkout -B "$BRANCH"
          git add data/edges.json data/units.json
          git commit -m "scraper: add ${{ steps.run_scraper.outputs.NEW_EDGES }} new edges"
          git push -f origin "$BRANCH"
          gh pr create \
            --title "Scraper: ${{ steps.run_scraper.outputs.NEW_EDGES }} new edges ($(date +%Y-%m-%d))" \
            --body "Automatically extracted by daily scraper. All edges have \`verified: false\`. New units (if any) are also included — review both files before merging." \
            --base main \
            --head "$BRANCH" || \
            echo "PR already exists for $BRANCH — skipping creation"
